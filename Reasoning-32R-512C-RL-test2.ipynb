{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import autoaugment\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import contextlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(\n",
    "    batch_size: int         = 256, \n",
    "    num_workers: int        = 8,  \n",
    "    pin_memory: bool        = True,\n",
    "    persistent_workers: bool = True, \n",
    "):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        autoaugment.AutoAugment(autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers,\n",
    "        drop_last=True,              \n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers,\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.global_avg_pool(x)\n",
    "        w = self.relu(self.fc1(w))\n",
    "        w = 2 * self.sigmoid(self.fc2(w))\n",
    "        return x * w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, max_depth=3, reduction_factor=4):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        reduced_channels = in_channels // reduction_factor\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, reduced_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(1, in_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(reduced_channels, reduced_channels, kernel_size=3, stride=1, padding=1, bias=False, groups=reduced_channels)\n",
    "        self.bn2 = nn.GroupNorm(1, reduced_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(reduced_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.GroupNorm(1, reduced_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.skip_bn = nn.GroupNorm(1, in_channels)\n",
    "\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x, depth):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out = self.se(out)\n",
    "\n",
    "        skip = self.relu(identity)\n",
    "        skip = self.skip_bn(skip)\n",
    "        skip = self.skip_conv(skip)\n",
    "\n",
    "        out += skip\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmplifyConv(nn.Module):\n",
    "    def __init__(self, channels, max_depth=3, num_module=2):\n",
    "        super(AmplifyConv, self).__init__()\n",
    "        self.max_depth = max_depth\n",
    "        self.num_module = num_module\n",
    "\n",
    "        self.conv = nn.ModuleList([\n",
    "            BottleneckBlock(channels, channels, max_depth) for _ in range(num_module)\n",
    "        ])\n",
    "        self.bn = nn.GroupNorm(1, channels)\n",
    "\n",
    "        self.step_embeddings = nn.Parameter(torch.randn(max_depth, channels, 1, 1))\n",
    "        self.step_embeddings1 = nn.Parameter(torch.randn(max_depth, channels, 1, 1))\n",
    "        self.step_embeddings2 = nn.Parameter(torch.randn(max_depth, channels, 1, 1))\n",
    "\n",
    "    def step_forward(self, x, x_prev, depth):\n",
    "        out = self.conv[depth // (self.max_depth // self.num_module)](x, depth)\n",
    "        x = 2 * F.sigmoid(self.step_embeddings1[depth]) * F.relu(x) + (1 + self.step_embeddings[depth]) * out\n",
    "        x = self.bn(x)\n",
    "\n",
    "        if depth < self.max_depth - 1:\n",
    "            x = x + 2 * F.sigmoid(self.step_embeddings2[depth]) * F.relu(x_prev)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        prev = []\n",
    "        for d in range(self.max_depth):\n",
    "            if d % 2 == 0:\n",
    "                prev.append(x)\n",
    "                \n",
    "            x_prev = prev[((d + 1) - ((d + 1) & -(d + 1))) // 2]\n",
    "            x = checkpoint(self.step_forward, x, x_prev, d, use_reentrant=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursionAmplifyConv(nn.Module):\n",
    "    def __init__(self, channels, height, width):\n",
    "        super(RecursionAmplifyConv, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(3, channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.amconv = AmplifyConv(channels, max_depth=32, num_module=1)\n",
    "        \n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv0(x)\n",
    "\n",
    "        x = self.amconv(x)\n",
    "\n",
    "        x = torch.cat([self.max_pool(x), self.avg_pool(x)], dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.reamconv= RecursionAmplifyConv(512, 32, 32)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.reamconv(x)\n",
    "        \n",
    "        x = x.contiguous().view(-1, 1024)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_fine_tune:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def compute_reconstruct_reward(self, outputs, targets):\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct = (predictions == targets).float()\n",
    "        \n",
    "        rewards = (correct - 0.5)\n",
    "    \n",
    "        return rewards, correct.sum().item()\n",
    "\n",
    "    def compute_positive_reward(self, outputs, targets):\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct = (predictions == targets).float()\n",
    "        \n",
    "        rewards = correct\n",
    "    \n",
    "        return rewards, correct.sum().item()\n",
    "\n",
    "    def compute_negative_reward(self, outputs, targets):\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct = (predictions == targets).float()\n",
    "        \n",
    "        rewards = (correct - 1)\n",
    "    \n",
    "        return rewards, correct.sum().item()\n",
    "\n",
    "    \n",
    "    def emergency_policy_loss(self, ref_outputs, pre_outputs, outputs, targets):\n",
    "        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        select = - F.log_softmax(outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        entropy = torch.sum(probs * torch.log(probs), dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_probs = F.softmax(outputs, dim=1)\n",
    "            old_select = - F.log_softmax(outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "            old_entropy = torch.sum(old_probs * torch.log(old_probs), dim=1)\n",
    "\n",
    "        pre_probs = F.softmax(pre_outputs, dim=1)\n",
    "        pre_select = - F.log_softmax(pre_outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        pre_entropy = torch.sum(pre_probs * torch.log(pre_probs), dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_pre_probs = F.softmax(pre_outputs, dim=1)\n",
    "            old_pre_select = - F.log_softmax(pre_outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "            old_pre_entropy = torch.sum(old_pre_probs * torch.log(old_pre_probs), dim=1)\n",
    "        \n",
    "        ref_probs = F.softmax(ref_outputs, dim=1)\n",
    "        ref_select = - F.log_softmax(ref_outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        ref_entropy = torch.sum(ref_probs * torch.log(ref_probs), dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_ref_probs = F.softmax(ref_outputs, dim=1)\n",
    "            old_ref_select = - F.log_softmax(ref_outputs, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "            old_ref_entropy = torch.sum(old_ref_probs * torch.log(old_ref_probs), dim=1)\n",
    "\n",
    "        rewards, correct = self.compute_reconstruct_reward(outputs, targets)\n",
    "        p_rewards, _ = self.compute_positive_reward(outputs, targets)\n",
    "        n_rewards, _ = self.compute_negative_reward(outputs, targets)\n",
    "\n",
    "        ce_loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "        select_delta = (select - old_select)\n",
    "        pre_select_delta = (pre_select - old_pre_select)\n",
    "        ref_select_delta = (ref_select - old_ref_select)\n",
    "\n",
    "        entropy_delta = (entropy - old_entropy)\n",
    "        pre_entropy_delta = (pre_entropy - old_pre_entropy)\n",
    "        ref_entropy_delta = (ref_entropy - old_ref_entropy)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            metrics = {\n",
    "                'ce_loss': ce_loss.item(),\n",
    "                'targets': select.sum().item(),\n",
    "                'entropy': entropy.sum().item(),\n",
    "            }\n",
    "    \n",
    "        return select_delta, pre_select_delta, ref_select_delta, entropy_delta, pre_entropy_delta, ref_entropy_delta, correct, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    net: torch.nn.Module,\n",
    "    get_cifar10_loaders,\n",
    "    evaluate_model,\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 300,\n",
    "    eval_interval: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    checkpoint_path: str = \"Reasoning_32R_512C_RL.pth\",\n",
    "):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "    ref_net = copy.deepcopy(net).to(device)\n",
    "    pre_net = copy.deepcopy(net).to(device)\n",
    "\n",
    "    trainloader, testloader = get_cifar10_loaders(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    all_params = list(net.parameters()) + list(pre_net.parameters())\n",
    "    optimizer = torch.optim.Adam(all_params, lr=lr)\n",
    "    scaler = torch.amp.GradScaler(enabled=device.type == \"cuda\")\n",
    "    best_test_acc = 0.0\n",
    "    ref_accuracy = 0.0\n",
    "    pre_accuracy = 0.0\n",
    "    \n",
    "    rl_trainer = RL_fine_tune(device)\n",
    "    rl_metrics_accumulator = {}\n",
    "    prev_ref = []\n",
    "    ref_count = 32\n",
    "    d = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        batch_count = 0\n",
    "        running_cor = 0.0\n",
    "            \n",
    "        if (epoch == 0):\n",
    "            print(\"\\nðŸ’« starting supervised training and RL fine-tuning with 1e-3 learning rate\\n\")\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=300, eta_min=1e-6\n",
    "            )\n",
    "\n",
    "        rl_metrics_accumulator = {\n",
    "            'ce_loss': 0.0, 'targets': 0.0, 'entropy': 0.0,\n",
    "            'rewards_mean': 0.0, 'rewards_std': 0.0, \n",
    "        }\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = net(inputs)\n",
    "                ref_outputs = ref_net(inputs)\n",
    "                pre_outputs = pre_net(inputs)\n",
    "                \n",
    "                select_delta, pre_select_delta, ref_select_delta, entropy_delta, pre_entropy_delta, ref_entropy_delta, cor, metrics = rl_trainer.emergency_policy_loss(ref_outputs, pre_outputs, outputs, targets)\n",
    "\n",
    "                select_ratio = F.relu(pre_select_delta) + F.sigmoid(select_delta) - F.sigmoid(ref_select_delta)\n",
    "                select_loss = select_ratio.sum()\n",
    "                \n",
    "                entropy_ratio = F.relu(pre_entropy_delta) +  F.sigmoid(entropy_delta) - F.sigmoid(ref_entropy_delta)\n",
    "                entropy_loss = entropy_ratio.sum()\n",
    "                \n",
    "                loss = select_loss + entropy_loss\n",
    "                    \n",
    "                for key, value in metrics.items():\n",
    "                    rl_metrics_accumulator[key] += value\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(all_params, max_norm=0.5)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_cor += cor\n",
    "            batch_count += inputs.shape[0]\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_acc = running_cor / batch_count\n",
    "        avg_metrics = {k: v / batch_count for k, v in rl_metrics_accumulator.items()}\n",
    "            \n",
    "        print(f\"Epoch [{epoch + 1}] ðŸ“Š acc: {avg_acc:.3f} | Targets: {avg_metrics['targets']:.4f} | Entropy: {avg_metrics['entropy']:.4f}\")\n",
    "\n",
    "        if running_cor > ref_accuracy:\n",
    "            ref_accuracy = running_cor\n",
    "            ref = copy.deepcopy(net).to(device)\n",
    "            if d % 2 == 0:\n",
    "                prev_ref.append(ref)\n",
    "                prev_ref = prev_ref[-ref_count:]\n",
    "            ref_net = prev_ref[((d % ref_count + 1) - ((d % ref_count + 1) & -(d % ref_count + 1))) // 2]\n",
    "            d += 1\n",
    "            print(f\"New ref model saved\")\n",
    "\n",
    "        if running_cor > pre_accuracy:\n",
    "            pre_accuracy = running_cor\n",
    "            pre_net = copy.deepcopy(net).to(device)\n",
    "            \n",
    "        if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:\n",
    "            print(f\"\\nEvaluating at epoch {epoch + 1} â€¦\")\n",
    "            train_acc = evaluate_model(net, trainloader, criterion, \"Train\", device)\n",
    "            test_acc  = evaluate_model(net, testloader,  criterion, \"Test\",  device)\n",
    "            print(f\"Total Trainable Parameters: {count_parameters(net):,}\\n\")\n",
    "            print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                torch.save(net.state_dict(), checkpoint_path)\n",
    "                print(f\"New best model saved with test accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    print(f\"Best test accuracy achieved: {best_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    net: torch.nn.Module,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    dataset_name: str = \"\",\n",
    "    device=None,\n",
    "):\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    was_training = net.training\n",
    "    net.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    autocast_ctx = (torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "                    if use_bf16 else contextlib.nullcontext())\n",
    "\n",
    "    with torch.no_grad(), autocast_ctx:\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = net(images)  \n",
    "            batch_loss = criterion(outputs.float(), labels)\n",
    "\n",
    "            if not torch.isfinite(batch_loss):\n",
    "                print(\"[eval/warn] non-finite loss\",\n",
    "                      \"logits_minmax=\", float(outputs.min()), float(outputs.max()))\n",
    "                continue\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            loss_sum    += batch_loss.item() * bs\n",
    "            total_seen  += bs\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "\n",
    "    if total_seen == 0:\n",
    "        avg_loss = float(\"nan\")\n",
    "        acc = 0.0\n",
    "    else:\n",
    "        avg_loss = loss_sum / total_seen\n",
    "        acc = 100.0 * total_correct / total_seen\n",
    "\n",
    "    print(f\"{dataset_name} Accuracy: {acc:.2f}%\")\n",
    "    print(f\"{dataset_name} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if was_training:\n",
    "        net.train()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 515,754\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "model = Net()\n",
    "print(f\"Total Trainable Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "net = Net()\n",
    "train_model(net, get_cifar10_loaders, evaluate_model)\n",
    "\n",
    "model = Net()\n",
    "print(f\"Total Trainable Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
