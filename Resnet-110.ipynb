{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import autoaugment\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(\n",
    "    batch_size: int         = 256,   # use the VRAM you freed with AMP\n",
    "    num_workers: int        = 8,     # 4‑8 is a sweet‑spot on most GPUs\n",
    "    pin_memory: bool        = True,\n",
    "    persistent_workers: bool = True, # keeps the workers alive between epochs\n",
    "):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        autoaugment.AutoAugment(autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers,\n",
    "        drop_last=True,                # keeps every batch full for BN / AMP\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers,\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        x = x.permute(0, 2, 3, 1) # [B, H, W, C] \n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2) # [B, C, H, W]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet110(nn.Module):\n",
    "    def __init__(self, num_blocks=[18, 18, 18], num_classes=10):\n",
    "        super(ResNet110, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # 3 stages with 18 blocks each (18*2*3 + 2 = 110 layers)\n",
    "        self.layer1 = self._make_layer(16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(64, num_blocks[2], stride=2)\n",
    "        \n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, 0, 0.01)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(num_classes=10):\n",
    "    \"\"\"Constructs a ResNet-110 model for CIFAR datasets.\"\"\"\n",
    "    return ResNet110(num_blocks=[18, 18, 18], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    net: torch.nn.Module,\n",
    "    get_cifar10_loaders,  # existing dataloader factory\n",
    "    evaluate_model,       # existing eval helper\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 300,\n",
    "    lr: float = 1e-3,\n",
    "    eval_interval: int = 10,\n",
    "    checkpoint_path: str = \"model_checkpoint_recursion16.pth\",\n",
    "):\n",
    "    \"\"\"Speed‑tuned training loop with per‑epoch timing & throughput report.\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # compile once for graph fusion\n",
    "    net = torch.compile(net, mode=\"reduce-overhead\").to(device)\n",
    "\n",
    "    # data\n",
    "    trainloader, testloader = get_cifar10_loaders(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    # optimisation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # new AMP API\n",
    "    scaler = torch.amp.GradScaler(enabled=device.type == \"cuda\")\n",
    "\n",
    "    best_test_acc = 0.0\n",
    "    num_samples = len(trainloader.dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # forward – mixed precision\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # backward + step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # ensure accurate timing for CUDA\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_time = time.perf_counter() - start_time\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        throughput = num_samples / epoch_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:03d}/{epochs} ─ {epoch_time:.2f}s | \"\n",
    "            f\"loss {avg_loss:.3f} | {throughput:.1f} img/s\"\n",
    "        )\n",
    "\n",
    "        # evaluation & checkpointing\n",
    "        if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:\n",
    "            print(f\"\\nEvaluating at epoch {epoch + 1} …\")\n",
    "            train_acc = evaluate_model(net, trainloader, criterion, \"Train\", device)\n",
    "            test_acc = evaluate_model(net, testloader, criterion, \"Test\", device)\n",
    "            print(\n",
    "                f\"Current learning rate: {optimizer.param_groups[0]['lr']:.2e}\\n\"\n",
    "            )\n",
    "\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"model_state_dict\": net.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    checkpoint_path,\n",
    "                )\n",
    "                print(\n",
    "                    f\"New best model saved with test accuracy: {test_acc:.2f}%\"\n",
    "                )\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    print(f\"Best test accuracy achieved: {best_test_acc:.2f}%\")\n",
    "    return best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    net: torch.nn.Module,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    dataset_name: str = \"\",\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"Run inference on *dataloader* under mixed precision and report accuracy.\"\"\"\n",
    "\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    was_training = net.training  # remember current mode\n",
    "    net.eval()\n",
    "\n",
    "    correct = total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = net(images)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"{dataset_name} Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"{dataset_name} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if was_training:\n",
    "        net.train()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 1,730,714\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "model = Net()\n",
    "print(f\"Total Trainable Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] loss: 1.967\n",
      "Epoch [2] loss: 1.638\n",
      "Epoch [3] loss: 1.438\n",
      "Epoch [4] loss: 1.284\n",
      "Epoch [5] loss: 1.156\n",
      "Epoch [6] loss: 1.051\n",
      "Epoch [7] loss: 0.976\n",
      "Epoch [8] loss: 0.911\n",
      "Epoch [9] loss: 0.864\n",
      "Epoch [10] loss: 0.818\n",
      "\n",
      "Evaluating at epoch 10 …\n",
      "Train Accuracy: 67.34%\n",
      "Train Average Loss: 0.9495\n",
      "Test Accuracy: 73.90%\n",
      "Test Average Loss: 0.7733\n",
      "Current learning rate: 9.97e-04\n",
      "\n",
      "New best model saved with test accuracy: 73.90%\n",
      "Epoch [11] loss: 0.785\n",
      "Epoch [12] loss: 0.748\n",
      "Epoch [13] loss: 0.719\n",
      "Epoch [14] loss: 0.700\n",
      "Epoch [15] loss: 0.673\n",
      "Epoch [16] loss: 0.649\n",
      "Epoch [17] loss: 0.637\n",
      "Epoch [18] loss: 0.613\n",
      "Epoch [19] loss: 0.595\n",
      "Epoch [20] loss: 0.585\n",
      "\n",
      "Evaluating at epoch 20 …\n",
      "Train Accuracy: 75.33%\n",
      "Train Average Loss: 0.7251\n",
      "Test Accuracy: 79.45%\n",
      "Test Average Loss: 0.6370\n",
      "Current learning rate: 9.89e-04\n",
      "\n",
      "New best model saved with test accuracy: 79.45%\n",
      "Epoch [21] loss: 0.567\n",
      "Epoch [22] loss: 0.551\n",
      "Epoch [23] loss: 0.537\n",
      "Epoch [24] loss: 0.537\n",
      "Epoch [25] loss: 0.522\n",
      "Epoch [26] loss: 0.508\n",
      "Epoch [27] loss: 0.499\n",
      "Epoch [28] loss: 0.489\n",
      "Epoch [29] loss: 0.482\n",
      "Epoch [30] loss: 0.469\n",
      "\n",
      "Evaluating at epoch 30 …\n",
      "Train Accuracy: 82.48%\n",
      "Train Average Loss: 0.5019\n",
      "Test Accuracy: 86.00%\n",
      "Test Average Loss: 0.4318\n",
      "Current learning rate: 9.76e-04\n",
      "\n",
      "New best model saved with test accuracy: 86.00%\n",
      "Epoch [31] loss: 0.466\n",
      "Epoch [32] loss: 0.461\n",
      "Epoch [33] loss: 0.448\n",
      "Epoch [34] loss: 0.436\n",
      "Epoch [35] loss: 0.432\n",
      "Epoch [36] loss: 0.427\n",
      "Epoch [37] loss: 0.422\n",
      "Epoch [38] loss: 0.415\n",
      "Epoch [39] loss: 0.412\n",
      "Epoch [40] loss: 0.401\n",
      "\n",
      "Evaluating at epoch 40 …\n",
      "Train Accuracy: 85.16%\n",
      "Train Average Loss: 0.4197\n",
      "Test Accuracy: 86.63%\n",
      "Test Average Loss: 0.3977\n",
      "Current learning rate: 9.57e-04\n",
      "\n",
      "New best model saved with test accuracy: 86.63%\n",
      "Epoch [41] loss: 0.396\n",
      "Epoch [42] loss: 0.392\n",
      "Epoch [43] loss: 0.394\n",
      "Epoch [44] loss: 0.381\n",
      "Epoch [45] loss: 0.382\n",
      "Epoch [46] loss: 0.374\n",
      "Epoch [47] loss: 0.364\n",
      "Epoch [48] loss: 0.357\n",
      "Epoch [49] loss: 0.357\n",
      "Epoch [50] loss: 0.358\n",
      "\n",
      "Evaluating at epoch 50 …\n",
      "Train Accuracy: 87.01%\n",
      "Train Average Loss: 0.3770\n",
      "Test Accuracy: 87.84%\n",
      "Test Average Loss: 0.3835\n",
      "Current learning rate: 9.33e-04\n",
      "\n",
      "New best model saved with test accuracy: 87.84%\n",
      "Epoch [51] loss: 0.347\n",
      "Epoch [52] loss: 0.347\n",
      "Epoch [53] loss: 0.341\n",
      "Epoch [54] loss: 0.336\n",
      "Epoch [55] loss: 0.331\n",
      "Epoch [56] loss: 0.327\n",
      "Epoch [57] loss: 0.325\n",
      "Epoch [58] loss: 0.323\n",
      "Epoch [59] loss: 0.319\n",
      "Epoch [60] loss: 0.313\n",
      "\n",
      "Evaluating at epoch 60 …\n",
      "Train Accuracy: 88.43%\n",
      "Train Average Loss: 0.3314\n",
      "Test Accuracy: 88.90%\n",
      "Test Average Loss: 0.3700\n",
      "Current learning rate: 9.05e-04\n",
      "\n",
      "New best model saved with test accuracy: 88.90%\n",
      "Epoch [61] loss: 0.315\n",
      "Epoch [62] loss: 0.308\n",
      "Epoch [63] loss: 0.299\n",
      "Epoch [64] loss: 0.302\n",
      "Epoch [65] loss: 0.293\n",
      "Epoch [66] loss: 0.303\n",
      "Epoch [67] loss: 0.297\n",
      "Epoch [68] loss: 0.292\n",
      "Epoch [69] loss: 0.290\n",
      "Epoch [70] loss: 0.288\n",
      "\n",
      "Evaluating at epoch 70 …\n",
      "Train Accuracy: 90.19%\n",
      "Train Average Loss: 0.2834\n",
      "Test Accuracy: 89.91%\n",
      "Test Average Loss: 0.3453\n",
      "Current learning rate: 8.72e-04\n",
      "\n",
      "New best model saved with test accuracy: 89.91%\n",
      "Epoch [71] loss: 0.283\n",
      "Epoch [72] loss: 0.281\n",
      "Epoch [73] loss: 0.275\n",
      "Epoch [74] loss: 0.268\n",
      "Epoch [75] loss: 0.268\n",
      "Epoch [76] loss: 0.267\n",
      "Epoch [77] loss: 0.268\n",
      "Epoch [78] loss: 0.264\n",
      "Epoch [79] loss: 0.263\n",
      "Epoch [80] loss: 0.257\n",
      "\n",
      "Evaluating at epoch 80 …\n",
      "Train Accuracy: 90.15%\n",
      "Train Average Loss: 0.2834\n",
      "Test Accuracy: 89.92%\n",
      "Test Average Loss: 0.3353\n",
      "Current learning rate: 8.35e-04\n",
      "\n",
      "New best model saved with test accuracy: 89.92%\n",
      "Epoch [81] loss: 0.258\n",
      "Epoch [82] loss: 0.254\n",
      "Epoch [83] loss: 0.257\n",
      "Epoch [84] loss: 0.246\n",
      "Epoch [85] loss: 0.248\n",
      "Epoch [86] loss: 0.250\n",
      "Epoch [87] loss: 0.252\n",
      "Epoch [88] loss: 0.242\n",
      "Epoch [89] loss: 0.238\n",
      "Epoch [90] loss: 0.240\n",
      "\n",
      "Evaluating at epoch 90 …\n",
      "Train Accuracy: 91.14%\n",
      "Train Average Loss: 0.2533\n",
      "Test Accuracy: 90.07%\n",
      "Test Average Loss: 0.3537\n",
      "Current learning rate: 7.94e-04\n",
      "\n",
      "New best model saved with test accuracy: 90.07%\n",
      "Epoch [91] loss: 0.239\n",
      "Epoch [92] loss: 0.237\n",
      "Epoch [93] loss: 0.238\n",
      "Epoch [94] loss: 0.232\n",
      "Epoch [95] loss: 0.232\n",
      "Epoch [96] loss: 0.230\n",
      "Epoch [97] loss: 0.231\n",
      "Epoch [98] loss: 0.218\n",
      "Epoch [99] loss: 0.228\n",
      "Epoch [100] loss: 0.219\n",
      "\n",
      "Evaluating at epoch 100 …\n",
      "Train Accuracy: 91.72%\n",
      "Train Average Loss: 0.2369\n",
      "Test Accuracy: 90.54%\n",
      "Test Average Loss: 0.3420\n",
      "Current learning rate: 7.50e-04\n",
      "\n",
      "New best model saved with test accuracy: 90.54%\n",
      "Epoch [101] loss: 0.222\n",
      "Epoch [102] loss: 0.220\n",
      "Epoch [103] loss: 0.215\n",
      "Epoch [104] loss: 0.214\n",
      "Epoch [105] loss: 0.215\n",
      "Epoch [106] loss: 0.206\n",
      "Epoch [107] loss: 0.213\n",
      "Epoch [108] loss: 0.212\n",
      "Epoch [109] loss: 0.206\n",
      "Epoch [110] loss: 0.207\n",
      "\n",
      "Evaluating at epoch 110 …\n",
      "Train Accuracy: 92.55%\n",
      "Train Average Loss: 0.2127\n",
      "Test Accuracy: 91.26%\n",
      "Test Average Loss: 0.3192\n",
      "Current learning rate: 7.04e-04\n",
      "\n",
      "New best model saved with test accuracy: 91.26%\n",
      "Epoch [111] loss: 0.206\n",
      "Epoch [112] loss: 0.198\n",
      "Epoch [113] loss: 0.204\n",
      "Epoch [114] loss: 0.198\n",
      "Epoch [115] loss: 0.202\n",
      "Epoch [116] loss: 0.198\n",
      "Epoch [117] loss: 0.194\n",
      "Epoch [118] loss: 0.195\n",
      "Epoch [119] loss: 0.196\n",
      "Epoch [120] loss: 0.196\n",
      "\n",
      "Evaluating at epoch 120 …\n",
      "Train Accuracy: 93.27%\n",
      "Train Average Loss: 0.1955\n",
      "Test Accuracy: 91.22%\n",
      "Test Average Loss: 0.3322\n",
      "Current learning rate: 6.55e-04\n",
      "\n",
      "Epoch [121] loss: 0.195\n",
      "Epoch [122] loss: 0.189\n",
      "Epoch [123] loss: 0.191\n",
      "Epoch [124] loss: 0.193\n",
      "Epoch [125] loss: 0.185\n",
      "Epoch [126] loss: 0.193\n",
      "Epoch [127] loss: 0.187\n",
      "Epoch [128] loss: 0.187\n",
      "Epoch [129] loss: 0.179\n",
      "Epoch [130] loss: 0.185\n",
      "\n",
      "Evaluating at epoch 130 …\n",
      "Train Accuracy: 93.38%\n",
      "Train Average Loss: 0.1896\n",
      "Test Accuracy: 90.99%\n",
      "Test Average Loss: 0.3394\n",
      "Current learning rate: 6.04e-04\n",
      "\n",
      "Epoch [131] loss: 0.180\n",
      "Epoch [132] loss: 0.178\n",
      "Epoch [133] loss: 0.180\n",
      "Epoch [134] loss: 0.179\n",
      "Epoch [135] loss: 0.178\n",
      "Epoch [136] loss: 0.175\n",
      "Epoch [137] loss: 0.177\n",
      "Epoch [138] loss: 0.174\n",
      "Epoch [139] loss: 0.170\n",
      "Epoch [140] loss: 0.167\n",
      "\n",
      "Evaluating at epoch 140 …\n",
      "Train Accuracy: 93.45%\n",
      "Train Average Loss: 0.1914\n",
      "Test Accuracy: 91.89%\n",
      "Test Average Loss: 0.3146\n",
      "Current learning rate: 5.53e-04\n",
      "\n",
      "New best model saved with test accuracy: 91.89%\n",
      "Epoch [141] loss: 0.177\n",
      "Epoch [142] loss: 0.174\n",
      "Epoch [143] loss: 0.169\n",
      "Epoch [144] loss: 0.165\n",
      "Epoch [145] loss: 0.167\n",
      "Epoch [146] loss: 0.164\n",
      "Epoch [147] loss: 0.164\n",
      "Epoch [148] loss: 0.161\n",
      "Epoch [149] loss: 0.163\n",
      "Epoch [150] loss: 0.164\n",
      "\n",
      "Evaluating at epoch 150 …\n",
      "Train Accuracy: 94.14%\n",
      "Train Average Loss: 0.1669\n",
      "Test Accuracy: 91.42%\n",
      "Test Average Loss: 0.3385\n",
      "Current learning rate: 5.01e-04\n",
      "\n",
      "Epoch [151] loss: 0.163\n",
      "Epoch [152] loss: 0.163\n",
      "Epoch [153] loss: 0.159\n",
      "Epoch [154] loss: 0.158\n",
      "Epoch [155] loss: 0.162\n",
      "Epoch [156] loss: 0.157\n",
      "Epoch [157] loss: 0.158\n",
      "Epoch [158] loss: 0.159\n",
      "Epoch [159] loss: 0.153\n",
      "Epoch [160] loss: 0.152\n",
      "\n",
      "Evaluating at epoch 160 …\n",
      "Train Accuracy: 94.74%\n",
      "Train Average Loss: 0.1542\n",
      "Test Accuracy: 91.83%\n",
      "Test Average Loss: 0.3087\n",
      "Current learning rate: 4.48e-04\n",
      "\n",
      "Epoch [161] loss: 0.156\n",
      "Epoch [162] loss: 0.152\n",
      "Epoch [163] loss: 0.151\n",
      "Epoch [164] loss: 0.150\n",
      "Epoch [165] loss: 0.150\n",
      "Epoch [166] loss: 0.151\n",
      "Epoch [167] loss: 0.143\n",
      "Epoch [168] loss: 0.149\n",
      "Epoch [169] loss: 0.146\n",
      "Epoch [170] loss: 0.150\n",
      "\n",
      "Evaluating at epoch 170 …\n",
      "Train Accuracy: 94.82%\n",
      "Train Average Loss: 0.1488\n",
      "Test Accuracy: 91.90%\n",
      "Test Average Loss: 0.3183\n",
      "Current learning rate: 3.97e-04\n",
      "\n",
      "New best model saved with test accuracy: 91.90%\n",
      "Epoch [171] loss: 0.148\n",
      "Epoch [172] loss: 0.146\n",
      "Epoch [173] loss: 0.146\n",
      "Epoch [174] loss: 0.141\n",
      "Epoch [175] loss: 0.141\n",
      "Epoch [176] loss: 0.143\n",
      "Epoch [177] loss: 0.140\n",
      "Epoch [178] loss: 0.142\n",
      "Epoch [179] loss: 0.141\n",
      "Epoch [180] loss: 0.142\n",
      "\n",
      "Evaluating at epoch 180 …\n",
      "Train Accuracy: 95.31%\n",
      "Train Average Loss: 0.1382\n",
      "Test Accuracy: 92.62%\n",
      "Test Average Loss: 0.3177\n",
      "Current learning rate: 3.46e-04\n",
      "\n",
      "New best model saved with test accuracy: 92.62%\n",
      "Epoch [181] loss: 0.140\n",
      "Epoch [182] loss: 0.138\n",
      "Epoch [183] loss: 0.136\n",
      "Epoch [184] loss: 0.133\n",
      "Epoch [185] loss: 0.140\n",
      "Epoch [186] loss: 0.135\n",
      "Epoch [187] loss: 0.136\n",
      "Epoch [188] loss: 0.134\n",
      "Epoch [189] loss: 0.132\n",
      "Epoch [190] loss: 0.136\n",
      "\n",
      "Evaluating at epoch 190 …\n",
      "Train Accuracy: 95.73%\n",
      "Train Average Loss: 0.1245\n",
      "Test Accuracy: 92.14%\n",
      "Test Average Loss: 0.3453\n",
      "Current learning rate: 2.97e-04\n",
      "\n",
      "Epoch [191] loss: 0.134\n",
      "Epoch [192] loss: 0.137\n",
      "Epoch [193] loss: 0.133\n",
      "Epoch [194] loss: 0.131\n",
      "Epoch [195] loss: 0.130\n",
      "Epoch [196] loss: 0.129\n",
      "Epoch [197] loss: 0.131\n",
      "Epoch [198] loss: 0.131\n",
      "Epoch [199] loss: 0.129\n",
      "Epoch [200] loss: 0.127\n",
      "\n",
      "Evaluating at epoch 200 …\n",
      "Train Accuracy: 95.78%\n",
      "Train Average Loss: 0.1250\n",
      "Test Accuracy: 92.27%\n",
      "Test Average Loss: 0.3352\n",
      "Current learning rate: 2.51e-04\n",
      "\n",
      "Epoch [201] loss: 0.129\n",
      "Epoch [202] loss: 0.124\n",
      "Epoch [203] loss: 0.124\n",
      "Epoch [204] loss: 0.122\n",
      "Epoch [205] loss: 0.129\n",
      "Epoch [206] loss: 0.124\n",
      "Epoch [207] loss: 0.121\n",
      "Epoch [208] loss: 0.124\n",
      "Epoch [209] loss: 0.125\n",
      "Epoch [210] loss: 0.126\n",
      "\n",
      "Evaluating at epoch 210 …\n",
      "Train Accuracy: 96.09%\n",
      "Train Average Loss: 0.1141\n",
      "Test Accuracy: 92.71%\n",
      "Test Average Loss: 0.3328\n",
      "Current learning rate: 2.07e-04\n",
      "\n",
      "New best model saved with test accuracy: 92.71%\n",
      "Epoch [211] loss: 0.120\n",
      "Epoch [212] loss: 0.121\n",
      "Epoch [213] loss: 0.121\n",
      "Epoch [214] loss: 0.120\n",
      "Epoch [215] loss: 0.122\n",
      "Epoch [216] loss: 0.126\n",
      "Epoch [217] loss: 0.122\n",
      "Epoch [218] loss: 0.119\n",
      "Epoch [219] loss: 0.120\n",
      "Epoch [220] loss: 0.118\n",
      "\n",
      "Evaluating at epoch 220 …\n",
      "Train Accuracy: 95.96%\n",
      "Train Average Loss: 0.1154\n",
      "Test Accuracy: 92.51%\n",
      "Test Average Loss: 0.3375\n",
      "Current learning rate: 1.66e-04\n",
      "\n",
      "Epoch [221] loss: 0.122\n",
      "Epoch [222] loss: 0.118\n",
      "Epoch [223] loss: 0.113\n",
      "Epoch [224] loss: 0.121\n",
      "Epoch [225] loss: 0.120\n",
      "Epoch [226] loss: 0.115\n",
      "Epoch [227] loss: 0.117\n",
      "Epoch [228] loss: 0.114\n",
      "Epoch [229] loss: 0.111\n",
      "Epoch [230] loss: 0.111\n",
      "\n",
      "Evaluating at epoch 230 …\n",
      "Train Accuracy: 96.05%\n",
      "Train Average Loss: 0.1137\n",
      "Test Accuracy: 92.77%\n",
      "Test Average Loss: 0.3262\n",
      "Current learning rate: 1.29e-04\n",
      "\n",
      "New best model saved with test accuracy: 92.77%\n",
      "Epoch [231] loss: 0.116\n",
      "Epoch [232] loss: 0.117\n",
      "Epoch [233] loss: 0.111\n",
      "Epoch [234] loss: 0.110\n",
      "Epoch [235] loss: 0.115\n",
      "Epoch [236] loss: 0.108\n",
      "Epoch [237] loss: 0.112\n",
      "Epoch [238] loss: 0.113\n",
      "Epoch [239] loss: 0.114\n",
      "Epoch [240] loss: 0.112\n",
      "\n",
      "Evaluating at epoch 240 …\n",
      "Train Accuracy: 96.26%\n",
      "Train Average Loss: 0.1082\n",
      "Test Accuracy: 92.70%\n",
      "Test Average Loss: 0.3242\n",
      "Current learning rate: 9.64e-05\n",
      "\n",
      "Epoch [241] loss: 0.111\n",
      "Epoch [242] loss: 0.114\n",
      "Epoch [243] loss: 0.111\n",
      "Epoch [244] loss: 0.113\n",
      "Epoch [245] loss: 0.108\n",
      "Epoch [246] loss: 0.107\n",
      "Epoch [247] loss: 0.109\n",
      "Epoch [248] loss: 0.109\n",
      "Epoch [249] loss: 0.110\n",
      "Epoch [250] loss: 0.106\n",
      "\n",
      "Evaluating at epoch 250 …\n",
      "Train Accuracy: 96.41%\n",
      "Train Average Loss: 0.1069\n",
      "Test Accuracy: 92.98%\n",
      "Test Average Loss: 0.3243\n",
      "Current learning rate: 6.79e-05\n",
      "\n",
      "New best model saved with test accuracy: 92.98%\n",
      "Epoch [251] loss: 0.108\n",
      "Epoch [252] loss: 0.105\n",
      "Epoch [253] loss: 0.112\n",
      "Epoch [254] loss: 0.102\n",
      "Epoch [255] loss: 0.108\n",
      "Epoch [256] loss: 0.108\n",
      "Epoch [257] loss: 0.109\n",
      "Epoch [258] loss: 0.107\n",
      "Epoch [259] loss: 0.106\n",
      "Epoch [260] loss: 0.109\n",
      "\n",
      "Evaluating at epoch 260 …\n",
      "Train Accuracy: 96.46%\n",
      "Train Average Loss: 0.1047\n",
      "Test Accuracy: 92.76%\n",
      "Test Average Loss: 0.3251\n",
      "Current learning rate: 4.42e-05\n",
      "\n",
      "Epoch [261] loss: 0.104\n",
      "Epoch [262] loss: 0.105\n",
      "Epoch [263] loss: 0.107\n",
      "Epoch [264] loss: 0.109\n",
      "Epoch [265] loss: 0.104\n",
      "Epoch [266] loss: 0.107\n",
      "Epoch [267] loss: 0.103\n",
      "Epoch [268] loss: 0.105\n",
      "Epoch [269] loss: 0.104\n",
      "Epoch [270] loss: 0.107\n",
      "\n",
      "Evaluating at epoch 270 …\n",
      "Train Accuracy: 96.49%\n",
      "Train Average Loss: 0.1023\n",
      "Test Accuracy: 92.96%\n",
      "Test Average Loss: 0.3218\n",
      "Current learning rate: 2.54e-05\n",
      "\n",
      "Epoch [271] loss: 0.105\n",
      "Epoch [272] loss: 0.105\n",
      "Epoch [273] loss: 0.105\n",
      "Epoch [274] loss: 0.102\n",
      "Epoch [275] loss: 0.105\n",
      "Epoch [276] loss: 0.102\n",
      "Epoch [277] loss: 0.105\n",
      "Epoch [278] loss: 0.101\n",
      "Epoch [279] loss: 0.104\n",
      "Epoch [280] loss: 0.102\n",
      "\n",
      "Evaluating at epoch 280 …\n",
      "Train Accuracy: 96.64%\n",
      "Train Average Loss: 0.0986\n",
      "Test Accuracy: 93.20%\n",
      "Test Average Loss: 0.3240\n",
      "Current learning rate: 1.19e-05\n",
      "\n",
      "New best model saved with test accuracy: 93.20%\n",
      "Epoch [281] loss: 0.101\n",
      "Epoch [282] loss: 0.102\n",
      "Epoch [283] loss: 0.105\n",
      "Epoch [284] loss: 0.106\n",
      "Epoch [285] loss: 0.106\n",
      "Epoch [286] loss: 0.102\n",
      "Epoch [287] loss: 0.103\n",
      "Epoch [288] loss: 0.103\n",
      "Epoch [289] loss: 0.106\n",
      "Epoch [290] loss: 0.105\n",
      "\n",
      "Evaluating at epoch 290 …\n",
      "Train Accuracy: 96.55%\n",
      "Train Average Loss: 0.1017\n",
      "Test Accuracy: 93.03%\n",
      "Test Average Loss: 0.3226\n",
      "Current learning rate: 3.74e-06\n",
      "\n",
      "Epoch [291] loss: 0.105\n",
      "Epoch [292] loss: 0.101\n",
      "Epoch [293] loss: 0.104\n",
      "Epoch [294] loss: 0.104\n",
      "Epoch [295] loss: 0.104\n",
      "Epoch [296] loss: 0.104\n",
      "Epoch [297] loss: 0.107\n",
      "Epoch [298] loss: 0.103\n",
      "Epoch [299] loss: 0.102\n",
      "Epoch [300] loss: 0.103\n",
      "\n",
      "Evaluating at epoch 300 …\n",
      "Train Accuracy: 96.54%\n",
      "Train Average Loss: 0.1014\n",
      "Test Accuracy: 93.02%\n",
      "Test Average Loss: 0.3241\n",
      "Current learning rate: 1.00e-06\n",
      "\n",
      "Finished Training\n",
      "Best test accuracy achieved: 93.20%\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "net = Net()\n",
    "train_model(net, get_cifar10_loaders, evaluate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
